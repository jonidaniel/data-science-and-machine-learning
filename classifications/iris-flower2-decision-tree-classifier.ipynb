{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f856d079",
   "metadata": {},
   "source": [
    "## Part 2 - Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88af3393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the scaled data frame from Part 1 for reuse\n",
    "%store -r scaled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1180131e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44eb9555",
   "metadata": {},
   "source": [
    "### Prepare the x and y values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7363709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x will be used as input (sepal length (cm), sepal width (cm), petal length (cm), petal width (cm)),\n",
    "# y will be output (target)\n",
    "\n",
    "# Select all rows, columns up until the fourth column (sepal length (cm), sepal width (cm), petal length (cm), petal width (cm))\n",
    "X = scaled_df.values[:, :4].astype(np.float32) # Make sure that the type is float32\n",
    "# Select all rows and only the fifth column (target)\n",
    "y = scaled_df.values[:, 4].astype(int) # Make sure that the type is int"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef207b5",
   "metadata": {},
   "source": [
    "### Do the train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b91d230c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'X_train' (ndarray)\n",
      "Stored 'X_test' (ndarray)\n",
      "Stored 'y_train' (ndarray)\n",
      "Stored 'y_test' (ndarray)\n",
      "(120, 4) (30, 4) (120,) (30,)\n"
     ]
    }
   ],
   "source": [
    "# Define the test set to be 20 % (the train set will be 80 %)\n",
    "# Fix the split randomness with random_state = 1 (omit if you want it to be random)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .2, random_state = 1)\n",
    "\n",
    "# Store the training and testing sets so they can be used also in Part 3 - K-Nearest Neighbors Classifier\n",
    "%store X_train\n",
    "%store X_test\n",
    "%store y_train\n",
    "%store y_test\n",
    "\n",
    "# There'll be 120 training points and 30 testing points\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ed42ad",
   "metadata": {},
   "source": [
    "### Build the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e01c3081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9666666666666667"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the classifier\n",
    "clf = DecisionTreeClassifier(random_state = 1)\n",
    "# Train the classifier and then score it with the test set\n",
    "clf.fit(X_train, y_train).score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a5fea8",
   "metadata": {},
   "source": [
    "### Randomized search predictions and scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "128f202e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'splitter': 'random', 'min_samples_split': np.int64(4), 'max_features': 'log2', 'max_depth': np.int32(85), 'criterion': 'entropy'} \n",
      "\n",
      "Train accuracy: 0.9666666666666668\n",
      "Test accuracy: 0.9666666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fosse/Desktop/python/classifications/classifications-venv/lib/python3.13/site-packages/sklearn/model_selection/_validation.py:516: FitFailedWarning: \n",
      "15 fits failed out of a total of 50.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "15 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/fosse/Desktop/python/classifications/classifications-venv/lib/python3.13/site-packages/sklearn/model_selection/_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/fosse/Desktop/python/classifications/classifications-venv/lib/python3.13/site-packages/sklearn/base.py\", line 1358, in wrapper\n",
      "    estimator._validate_params()\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/Users/fosse/Desktop/python/classifications/classifications-venv/lib/python3.13/site-packages/sklearn/base.py\", line 471, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        self._parameter_constraints,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        self.get_params(deep=False),\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        caller_name=self.__class__.__name__,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/Users/fosse/Desktop/python/classifications/classifications-venv/lib/python3.13/site-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "    ...<2 lines>...\n",
      "    )\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of DecisionTreeClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/fosse/Desktop/python/classifications/classifications-venv/lib/python3.13/site-packages/sklearn/model_selection/_search.py:1135: UserWarning: One or more of the test scores are non-finite: [       nan 0.95       0.91666667 0.95       0.95       0.96666667\n",
      " 0.95              nan        nan 0.925     ]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# We're now trying to find out what'd be the optimal solution (i.e. optimal set of parameters) for the training\n",
    "\n",
    "# Set hyperparameters to fine-tune the decision tree\n",
    "# criterion: instructions on how to split your nodes within the decision tree (trying with gini and entropy)\n",
    "# splitter: how the nodes are going to be split (trying with best and random)\n",
    "# max_depth: how deep the tree is, i.e., how far it'll go with all of its nodes (trying with integers 5, 10, 15, etc., all the way up to 90)\n",
    "# min_samples_split: minimum number of samples to split (trying with integers 2, 3, 4, etc., up to 10)\n",
    "# max_features: maximum number of features that we're going to be looking at within the trees (trying with auto, square root, and base-2 logarithm)\n",
    "params = {\"criterion\": [\"gini\", \"entropy\"],\n",
    "          \"splitter\": [\"best\", \"random\"],\n",
    "          \"max_depth\": np.linspace(5, 90, 18).astype(np.int32),\n",
    "          \"min_samples_split\": np.arange(2, 10),\n",
    "          \"max_features\": [\"auto\", \"sqrt\", \"log2\"]}\n",
    "\n",
    "# Use the randomized search\n",
    "# It'll randomly take a selection of the hyperparametes and try to find which ones that work out the best\n",
    "# scoring = \"accuracy\" defines that the search will select the best parameters based on accuracy\n",
    "# cv = 5 stands for 5-fold cross-validation\n",
    "rand_search = RandomizedSearchCV(DecisionTreeClassifier(random_state = 1), params, scoring = \"accuracy\", random_state = 1, cv = 5)\n",
    "# Train the search model\n",
    "rand_search.fit(X_train, y_train)\n",
    "\n",
    "# The most optimal parameters the model found\n",
    "rand_params = rand_search.best_params_\n",
    "# Print the optimal parameters the model found\n",
    "print(rand_params, \"\\n\")\n",
    "\n",
    "# Print the best training accuracy, i.e., score the model found\n",
    "print(\"Train accuracy:\", rand_search.best_score_)\n",
    "# Predict with the test data (will be iris species, 0, 1, or 2)\n",
    "preds = rand_search.predict(X_test)\n",
    "# Print the testing accuracy\n",
    "print(\"Test accuracy:\", accuracy_score(preds, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412bf3f0",
   "metadata": {},
   "source": [
    "### Grid search predictions and scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2414f082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'entropy', 'max_depth': np.int64(82), 'max_features': 'log2', 'min_samples_split': np.int64(4), 'splitter': 'random'} \n",
      "\n",
      "Train accuracy: 0.9666666666666668\n",
      "Test accuracy: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "# Opposed to randomized search,\n",
    "# grid search will try out every single combination of hyperparameters it's given\n",
    "\n",
    "# max_depth and min_samples_split were the only numeric parameters\n",
    "# For those parameters, let's take the values the randomized search gave us,\n",
    "# and make the parameters new ranges from 3 below to 3 over the values\n",
    "# The other parameters shall have the values the randomized search came back with\n",
    "\n",
    "# Define a new range for the tree max depth\n",
    "max_depth = np.arange(rand_params[\"max_depth\"] - 3, rand_params[\"max_depth\"] + 3)\n",
    "# Define a new range for the minimum number of samples to split\n",
    "min_samples_split = np.arange(rand_params[\"min_samples_split\"] - 3, rand_params[\"min_samples_split\"] + 3)\n",
    "\n",
    "# Use the criterion, splitter, and max_features values the randomized search ended up with\n",
    "# max_depth and min_samples_split should always stay positive\n",
    "params = {\"criterion\": [rand_params[\"criterion\"]],\n",
    "          \"splitter\": [rand_params[\"splitter\"]],\n",
    "          \"max_depth\": max_depth[max_depth >= 2],\n",
    "          \"min_samples_split\": min_samples_split[min_samples_split >= 2],\n",
    "          \"max_features\": [rand_params[\"max_features\"]]}\n",
    "\n",
    "# Note that this time GridSearchCV doesn't take random_state as a parameter\n",
    "# (don't mix with DecisionTreeClassifier parameters),\n",
    "# because grid search goes through every hyperparameter\n",
    "grid_search = GridSearchCV(DecisionTreeClassifier(random_state = 1), params, scoring = \"accuracy\", cv = 5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "grid_params = grid_search.best_params_\n",
    "print(grid_params, \"\\n\")\n",
    "\n",
    "print(\"Train accuracy:\", grid_search.best_score_)\n",
    "preds = grid_search.predict(X_test)\n",
    "print(\"Test accuracy:\", accuracy_score(preds, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e726f77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        11\n",
      "           1       0.92      1.00      0.96        12\n",
      "           2       1.00      0.86      0.92         7\n",
      "\n",
      "    accuracy                           0.97        30\n",
      "   macro avg       0.97      0.95      0.96        30\n",
      "weighted avg       0.97      0.97      0.97        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# How well the classification performed?\n",
    "print(classification_report(preds, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "classifications-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
