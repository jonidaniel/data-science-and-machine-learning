{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3a1d753",
   "metadata": {},
   "source": [
    "## Part 3 - K-Nearest Neighbors Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9e07a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the scaled data frame from Part 1 for reuse\n",
    "%store -r scaled_df\n",
    "\n",
    "# Import the training and testing sets from Part 2 for reuse\n",
    "%store -r X_train\n",
    "%store -r X_test\n",
    "%store -r y_train\n",
    "%store -r y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b77311be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112128fa",
   "metadata": {},
   "source": [
    "### Build the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "645a614b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the classifier\n",
    "clf = KNeighborsClassifier()\n",
    "# Train the classifier and then score it with the test set\n",
    "clf.fit(X_train, y_train).score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4e0d9e",
   "metadata": {},
   "source": [
    "### Randomized search predictions and scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f73225a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'weights': 'uniform', 'n_neighbors': np.int64(12), 'metric': 'euclidean', 'leaf_size': np.int64(140), 'algorithm': 'kd_tree'} \n",
      "\n",
      "Train accuracy: 0.9583333333333334\n",
      "Test accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# We're now trying to find out what'd be the optimal solution (i.e. optimal set of parameters) for the training\n",
    "\n",
    "# Set hyperparameters to fine-tune the k-nearest neighbors\n",
    "params = {\"n_neighbors\": np.arange(1, 15),\n",
    "          \"weights\": [\"uniform\", \"distance\"],\n",
    "          \"algorithm\": [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"],\n",
    "          \"metric\": [\"minkowski\", \"euclidean\", \"manhattan\", \"chebyshev\"], # Distance functions (i.e. different ways to evaluate distances between points)\n",
    "          \"leaf_size\": np.linspace(10, 150, 15).astype(int)}\n",
    "\n",
    "# Use the randomized search\n",
    "# It'll randomly take a selection of the hyperparametes and try to find which ones that work out the best\n",
    "# scoring = \"accuracy\" defines that the search will select the best parameters based on accuracy\n",
    "# cv = 5 stands for 5-fold cross-validation\n",
    "rand_search = RandomizedSearchCV(KNeighborsClassifier(), params, scoring = \"accuracy\", cv = 5)\n",
    "# Train the search model\n",
    "rand_search.fit(X_train, y_train)\n",
    "\n",
    "# The most optimal parameters the model found\n",
    "rand_params = rand_search.best_params_\n",
    "# Print the optimal parameters the model found\n",
    "print(rand_params, \"\\n\")\n",
    "\n",
    "# Print the best training accuracy, i.e., score the model found\n",
    "print(\"Train accuracy:\", rand_search.best_score_)\n",
    "# Predict with the test data (will be iris species, 0, 1, or 2)\n",
    "preds = rand_search.predict(X_test)\n",
    "# Print the testing accuracy\n",
    "print(\"Test accuracy:\", accuracy_score(preds, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975009bf",
   "metadata": {},
   "source": [
    "### Grid search predictions and scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0851a972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algorithm': 'kd_tree', 'leaf_size': np.int64(137), 'metric': 'euclidean', 'n_neighbors': np.int64(14), 'weights': 'uniform'} \n",
      "\n",
      "Train accuracy: 0.9666666666666668\n",
      "Test accuracy: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "# Opposed to randomized search,\n",
    "# grid search will try out every single combination of hyperparameters it's given\n",
    "\n",
    "# n_neighbors and leaf_samples were the only numeric parameters\n",
    "# For those parameters, let's take the values the randomized search gave us,\n",
    "# and make the parameters new ranges from 3 below to 3 over the values\n",
    "# The other parameters shall have the values the randomized search came back with\n",
    "\n",
    "# Define a new range for the number of neighbors\n",
    "n_neighbors = np.arange(rand_params[\"n_neighbors\"] - 3, rand_params[\"n_neighbors\"] + 3)\n",
    "# Define a new range for the leaf size\n",
    "leaf_size = np.arange(rand_params[\"leaf_size\"] - 3, rand_params[\"leaf_size\"] + 3)\n",
    "\n",
    "# Use the weights, algorithm, and metric values the randomized search ended up with\n",
    "params = {\"n_neighbors\": n_neighbors,\n",
    "          \"weights\": [rand_params[\"weights\"]],\n",
    "          \"algorithm\": [rand_params[\"algorithm\"]],\n",
    "          \"metric\": [rand_params[\"metric\"]],\n",
    "          \"leaf_size\": leaf_size}\n",
    "\n",
    "# Note that this time GridSearchCV doesn't take random_state as a parameter,\n",
    "# because grid search goes through every hyperparameter\n",
    "grid_search = GridSearchCV(KNeighborsClassifier(), params, scoring = \"accuracy\", cv = 5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "grid_params = grid_search.best_params_\n",
    "print(grid_params, \"\\n\")\n",
    "\n",
    "print(\"Train accuracy:\", grid_search.best_score_)\n",
    "preds = grid_search.predict(X_test)\n",
    "print(\"Test accuracy:\", accuracy_score(preds, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2195d7f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        13\n",
      "           1       1.00      0.80      0.89         5\n",
      "           2       0.92      1.00      0.96        12\n",
      "\n",
      "    accuracy                           0.97        30\n",
      "   macro avg       0.97      0.93      0.95        30\n",
      "weighted avg       0.97      0.97      0.97        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# How well the classification performed?\n",
    "print(classification_report(preds, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "classifications-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
