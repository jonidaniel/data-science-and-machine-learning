{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f856d079",
   "metadata": {},
   "source": [
    "## Part 2 - Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "88af3393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the scaled data frame from Part 1 for reuse\n",
    "%store -r scaled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "1180131e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44eb9555",
   "metadata": {},
   "source": [
    "### Prepare the x and y values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "a7363709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x will be used as input (sepal length (cm), sepal width (cm), petal length (cm), petal width (cm)),\n",
    "# y will be output (target)\n",
    "\n",
    "# Select all rows, columns up until the fourth column (sepal length (cm), sepal width (cm), petal length (cm), petal width (cm))\n",
    "X = scaled_df.values[:, :4].astype(np.float32) # Make sure that the type is float32\n",
    "# Select all rows and only the fifth column (target)\n",
    "y = scaled_df.values[:, 4].astype(int) # Make sure that the type is int"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef207b5",
   "metadata": {},
   "source": [
    "### Do the train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "b91d230c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'X_train' (ndarray)\n",
      "Stored 'X_test' (ndarray)\n",
      "Stored 'y_train' (ndarray)\n",
      "Stored 'y_test' (ndarray)\n",
      "(120, 4) (30, 4) (120,) (30,)\n"
     ]
    }
   ],
   "source": [
    "# Define the test set to be 20 % (the train set will be 80 %)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .2)\n",
    "\n",
    "# Store the training and testing sets so they can be used also in Part 3 - K-Nearest Neighbors Classifier\n",
    "%store X_train\n",
    "%store X_test\n",
    "%store y_train\n",
    "%store y_test\n",
    "\n",
    "# There'll be 120 training points and 30 testing points\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ed42ad",
   "metadata": {},
   "source": [
    "### Build the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "e01c3081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the classifier\n",
    "clf = DecisionTreeClassifier()\n",
    "# Train the classifier and then score it with the test set\n",
    "clf.fit(X_train, y_train).score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a5fea8",
   "metadata": {},
   "source": [
    "### Randomized search predictions and scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "128f202e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'splitter': 'random', 'min_samples_split': np.int64(8), 'max_depth': np.int32(60), 'criterion': 'gini'} \n",
      "\n",
      "Train accuracy: 0.9416666666666668\n",
      "Test accuracy: 0.9\n"
     ]
    }
   ],
   "source": [
    "# We're now trying to find out what'd be the optimal solution (i.e. optimal set of parameters) for the training\n",
    "\n",
    "# Set hyperparameters to fine-tune the decision tree\n",
    "# criterion: instructions on how to split your nodes within the decision tree (trying with gini and entropy)\n",
    "# splitter: how the nodes are going to be split (trying with best and random)\n",
    "# max_depth: how deep the tree is, i.e., how far it'll go with all of its nodes (trying with integers 5, 10, 15, etc., all the way up to 90)\n",
    "# min_samples_split: minimum number of samples to split (trying with integers 2, 3, 4, etc., up to 10)\n",
    "params = {\"criterion\": [\"gini\", \"entropy\"],\n",
    "          \"splitter\": [\"best\", \"random\"],\n",
    "          \"max_depth\": np.linspace(5, 90, 18).astype(np.int32),\n",
    "          \"min_samples_split\": np.arange(2, 10)}\n",
    "\n",
    "# Use the randomized search\n",
    "# It'll randomly take a selection of the hyperparametes and try to find which ones that work out the best\n",
    "# scoring = \"accuracy\" defines that the search will select the best parameters based on accuracy\n",
    "# cv = 5 stands for 5-fold cross-validation\n",
    "rand_search = RandomizedSearchCV(DecisionTreeClassifier(), params, scoring = \"accuracy\", cv = 5)\n",
    "# Train the search model\n",
    "rand_search.fit(X_train, y_train)\n",
    "\n",
    "# The most optimal parameters the model found\n",
    "rand_params = rand_search.best_params_\n",
    "# Print the optimal parameters the model found\n",
    "print(rand_params, \"\\n\")\n",
    "\n",
    "# Print the best training accuracy, i.e., score the model found\n",
    "print(\"Train accuracy:\", rand_search.best_score_)\n",
    "# Predict with the test data (will be iris species, 0, 1, or 2)\n",
    "preds = rand_search.predict(X_test)\n",
    "# Print the testing accuracy\n",
    "print(\"Test accuracy:\", accuracy_score(preds, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412bf3f0",
   "metadata": {},
   "source": [
    "### Grid search predictions and scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "2414f082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'gini', 'max_depth': np.int64(57), 'min_samples_split': np.int64(10), 'splitter': 'random'} \n",
      "\n",
      "Train accuracy: 0.9666666666666666\n",
      "Test accuracy: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "# Opposed to randomized search,\n",
    "# grid search will try out every single combination of hyperparameters it's given\n",
    "\n",
    "# max_depth and min_samples_split were the only numeric parameters\n",
    "# For those parameters, let's take the values the randomized search gave us,\n",
    "# and make the parameters new ranges from 3 below to 3 over the values\n",
    "# The other parameters shall have the values the randomized search came back with\n",
    "\n",
    "# Define a new range for the tree max depth\n",
    "max_depth = np.arange(rand_params[\"max_depth\"] - 3, rand_params[\"max_depth\"] + 3)\n",
    "# Define a new range for the minimum number of samples to split\n",
    "min_samples_split = np.arange(rand_params[\"min_samples_split\"] - 3, rand_params[\"min_samples_split\"] + 3)\n",
    "\n",
    "# Use the criterion and splitter values the randomized search ended up with\n",
    "# max_depth and min_samples_split should always stay positive\n",
    "params = {\"criterion\": [rand_params[\"criterion\"]],\n",
    "          \"splitter\": [rand_params[\"splitter\"]],\n",
    "          \"max_depth\": max_depth[max_depth >= 2],\n",
    "          \"min_samples_split\": min_samples_split[min_samples_split >= 2]}\n",
    "\n",
    "# Use the grid search\n",
    "# It goes through every hyperparameter and tries to find out which ones work out the best\n",
    "grid_search = GridSearchCV(DecisionTreeClassifier(), params, scoring = \"accuracy\", cv = 5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "grid_params = grid_search.best_params_\n",
    "print(grid_params, \"\\n\")\n",
    "\n",
    "print(\"Train accuracy:\", grid_search.best_score_)\n",
    "preds = grid_search.predict(X_test)\n",
    "print(\"Test accuracy:\", accuracy_score(preds, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "e726f77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        13\n",
      "           1       1.00      0.80      0.89         5\n",
      "           2       0.92      1.00      0.96        12\n",
      "\n",
      "    accuracy                           0.97        30\n",
      "   macro avg       0.97      0.93      0.95        30\n",
      "weighted avg       0.97      0.97      0.97        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# How well the classification performed?\n",
    "print(classification_report(preds, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "classifications-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
